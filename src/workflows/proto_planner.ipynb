{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b8bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.agents import create_agent\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import interrupt\n",
    "import getpass\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00bf253",
   "metadata": {},
   "source": [
    "### Response Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6c15935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class StudyPlanEvaluation(BaseModel):\n",
    "    \"\"\"\n",
    "    Full evaluation of a student's semester study plan by multiple agents.\n",
    "    \"\"\"\n",
    "    scheduling_score: int = Field(\n",
    "        description=\"Score out of 100 given by the Scheduling Agent.\"\n",
    "    )\n",
    "    alignment_score: int = Field(\n",
    "        description=\"Score out of 100 given by the Alignment Agent.\"\n",
    "    )\n",
    "    weighted_color: str = Field(\n",
    "        description=\"Final approval color code (red/yellow/green) given by main agent.\"\n",
    "    )\n",
    "    scheduling_reasoning: str = Field(\n",
    "        description=\"Brief reasoning from Scheduling Agent.\"\n",
    "    )\n",
    "    alignment_reasoning: str = Field(\n",
    "        description=\"Brief reasoning from Alignment Agent.\"\n",
    "    )\n",
    "    overall_recommendation: str = Field(\n",
    "        description=\"Joint summary or recommendation by main agent (optional).\"\n",
    "    )\n",
    "    workload_score: int = Field(\n",
    "        description=\"Score out of 100 given by the Workload Agent.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ebf58",
   "metadata": {},
   "source": [
    "### Evaluation score averaging tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2cda816",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def weighted_score_tool(scheduling_score:int, alignment_score:int, workload_score: int) -> str:\n",
    "    \"\"\"\n",
    "    Calculate the weighted average score and return a color code based on the score.\"\"\"\n",
    "    w_avg = (0.4*scheduling_score + 0.4*alignment_score + 0.2*workload_score)\n",
    "    if 0 <= w_avg <= 45:\n",
    "        return \"red\"\n",
    "    elif 46 <= w_avg <= 75:\n",
    "        return \"yellow\"\n",
    "    else:\n",
    "        return \"green\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914410e",
   "metadata": {},
   "source": [
    "### Dataframe query tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cfcab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27 entries, 0 to 26\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Course Code  27 non-null     object\n",
      " 1   Full Name    27 non-null     object\n",
      " 2   Description  27 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 780.0+ bytes\n",
      "None\n",
      "-----\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27 entries, 0 to 26\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Course Code         27 non-null     object\n",
      " 1   Department          27 non-null     object\n",
      " 2   Course Level        27 non-null     object\n",
      " 3   Possible Full Name  27 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 996.0+ bytes\n",
      "None\n",
      "-----\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17 entries, 0 to 16\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Date         17 non-null     object\n",
      " 1   09:00–11:00  17 non-null     object\n",
      " 2   13:00–15:00  17 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 540.0+ bytes\n",
      "None\n",
      "-----\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Day          5 non-null      object\n",
      " 1   08:30–10:00  5 non-null      object\n",
      " 2   10:15–11:45  5 non-null      object\n",
      " 3   12:00–13:30  5 non-null      object\n",
      " 4   13:45–15:15  5 non-null      object\n",
      " 5   15:30–17:00  5 non-null      object\n",
      "dtypes: object(6)\n",
      "memory usage: 372.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load your CSV into a pandas DataFrame\n",
    "course_description_path = 'course_description.csv'\n",
    "course_masterlist_path = 'course_masterlist.csv'\n",
    "exams_path = 'exams.csv'\n",
    "lectures_path = 'lectures.csv'\n",
    "\n",
    "course_description_df = pd.read_csv(course_description_path)\n",
    "course_masterlist_df = pd.read_csv(course_masterlist_path)\n",
    "exams_df = pd.read_csv(exams_path)\n",
    "lectures_df = pd.read_csv(lectures_path)\n",
    "\n",
    "def query_dataframe(df: pd.DataFrame,query: str) -> str:\n",
    "    try:\n",
    "        result = df.query(query).to_string()\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error executing query: {e}\"\n",
    "\n",
    "\n",
    "course_description_query = partial(query_dataframe, course_description_df)\n",
    "course_masterlist_query = partial(query_dataframe, course_masterlist_df)\n",
    "exams_query = partial(query_dataframe, exams_df)\n",
    "lectures_query = partial(query_dataframe, lectures_df)\n",
    "\n",
    "print(course_description_df.info())\n",
    "print(\"-----\")\n",
    "print(course_masterlist_df.info())\n",
    "print(\"-----\")\n",
    "print(exams_df.info())\n",
    "print(\"-----\")\n",
    "print(lectures_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dff7e3",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Sub-agent prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab17be9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a Study Plan: {study_plan}\n",
      "To extract semester schedule details for each selected course, use the exams, and lectures tables.\n",
      "Evaluate when lectures, exercises, and exams occur for selected courses.\n",
      "Score out of 100 based on:\n",
      "- Breaks (<1hr) between lectures/exercises\n",
      "- Exams too close (<3 days)\n",
      "- Overlaps/conflicts not tolerated\n",
      "Explain your score briefly.\n",
      "\n",
      "-----\n",
      "Given a study plan: {study_plan}\n",
      "Analyze selection of courses for the semester using the course_description and course_masterlist table and student's major/minor.\n",
      "Score out of 100. Assessment must account for one exploratory course.\n",
      "Explain your score briefly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_prompt(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "scheduling_prompt_text = load_prompt(\"schedule_prompt.txt\")\n",
    "alignment_prompt_text = load_prompt(\"alignment_prompt.txt\")\n",
    "print(scheduling_prompt_text)\n",
    "print(\"-----\")\n",
    "print(alignment_prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5577266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def course_description_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Query the course description dataframe. Accepts a pandas query string.\"\"\"\n",
    "    return query_dataframe(df=course_description_df,query=query)\n",
    "\n",
    "@tool\n",
    "def course_masterlist_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Query the course masterlist dataframe. Accepts a pandas query string.\"\"\"\n",
    "    return query_dataframe(df=course_masterlist_df,query=query)\n",
    "\n",
    "@tool\n",
    "def exams_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Query the exams dataframe. Accepts a pandas query string.\n",
    "    \"\"\"\n",
    "    return query_dataframe(df=course_masterlist_df,query=query)\n",
    "@tool\n",
    "def lectures_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Query the lectures dataframe. Accepts a pandas query string.\n",
    "    \"\"\"\n",
    "    return query_dataframe(df=lectures_df,query=query)\n",
    "\n",
    "\n",
    "# Scheduling agent prompt\n",
    "scheduling_prompt = ChatPromptTemplate.from_messages(('human',scheduling_prompt_text))\n",
    "\n",
    "# Alignment agent prompt\n",
    "alignment_prompt = ChatPromptTemplate.from_messages(('human',alignment_prompt_text))\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.0,\n",
    "    max_retries=2)  # Substitute with your preferred model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403e35a",
   "metadata": {},
   "source": [
    "### Sub-agent initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf9889de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define each agent as a LangChain AgentExecutor, with relevant tools\n",
    "scheduling_agent = create_agent(model=llm, tools=[exams_tool,lectures_tool])  # Add relevant table tools\n",
    "\n",
    "@tool(\"schedule_evaluator\", description=\"Assesses study plan schedule for breaks and conflicts.\")\n",
    "def call_scheduling_agent(study_plan: str) -> dict:\n",
    "    chain = scheduling_prompt | scheduling_agent\n",
    "    result = chain.invoke({\"study_plan\": study_plan})\n",
    "    # Ensure the output structure matches StudyPlanEvaluation (score and reasoning)\n",
    "    return {\n",
    "        \"score\": result.get(\"score\"),\n",
    "        \"reasoning\": result.get(\"reasoning\")\n",
    "    }\n",
    "\n",
    "alignment_agent = create_agent(model=llm, tools=[course_description_tool,course_masterlist_tool])  # Add relevant table tools\n",
    "@tool(\"alignment_evaluator\", description=\"Evaluates courses alignment with major/minor.\")\n",
    "def call_alignment_agent(study_plan: str) -> dict:\n",
    "    chain = alignment_prompt | alignment_agent\n",
    "    result = chain.invoke({\"study_plan\": study_plan})\n",
    "    # Ensure the output structure matches StudyPlanEvaluation (score and reasoning)\n",
    "    return {\n",
    "        \"score\": result.get(\"score\"),\n",
    "        \"reasoning\": result.get(\"reasoning\")\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34935d",
   "metadata": {},
   "source": [
    "### Main Agent init and Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed5f5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_agent_prompt_text = \"\"\"\n",
    "You are the study plan supervisor. For the provided study plan, call schedule_evaluator and alignment_evaluator as needed.\n",
    "Aggregate their results, call weighted_score, and provide an output containing:\n",
    "- scheduling_score, alignment_score, weighted_color\n",
    "- scheduling_reasoning, alignment_reasoning\n",
    "- overall_recommendation\n",
    "Additionally, evaluate a score based on the amount of courses selected called workload_score and make your evaluation based on that.\n",
    "Be critical and thorough in your evaluation, avoid being over optimistic.\n",
    "\"\"\"\n",
    "# Use from_messages to construct the ChatPromptTemplate (ChatPromptTemplate requires 'messages' when using constructor)\n",
    "main_agent_prompt = ChatPromptTemplate.from_messages([('system',main_agent_prompt_text),('human',\"Given a study plan: \\n{study_plan}\\n\")])\n",
    "\n",
    "main_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[call_scheduling_agent, call_alignment_agent, weighted_score_tool],\n",
    "    response_format=StudyPlanEvaluation\n",
    ")\n",
    "chain = main_agent_prompt | main_agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28204fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(study_plan: str):\n",
    "    result = chain.invoke({\"study_plan\": study_plan})\n",
    "    # Parse results into StudyPlanEvaluation\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5a11cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alignment_reasoning': 'The study plan is well-aligned with the major and '\n",
      "                        'minor, but could be improved with more relevant '\n",
      "                        'courses.',\n",
      " 'alignment_score': 80,\n",
      " 'overall_recommendation': 'The study plan is good, but could be improved with '\n",
      "                           'more relevant courses and better scheduling.',\n",
      " 'scheduling_reasoning': 'The schedule is manageable, but could be improved '\n",
      "                         'with more breaks and less conflicts.',\n",
      " 'scheduling_score': 70,\n",
      " 'weighted_color': 'yellow',\n",
      " 'workload_score': 60}\n"
     ]
    }
   ],
   "source": [
    "green_case = load_prompt(\"green_case.txt\")\n",
    "yellow_case = load_prompt(\"yellow_structured.txt\")\n",
    "red_case = load_prompt(\"red_structured.txt\")\n",
    "pprint(run_evaluation(green_case)['structured_response'].model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c526aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def weighted_score_tool_with_interrupt(scheduling_score:int, alignment_score:int, workload_score: int) -> str:\n",
    "    \"\"\"\n",
    "    Calculate the weighted average score and return a color code based on the score.\n",
    "    \"\"\"\n",
    "    w_avg = 0.4 * scheduling_score + 0.5 * alignment_score + 0.1 * workload_score\n",
    "    \n",
    "    if 0 <= w_avg <= 45:\n",
    "        return \"red\"\n",
    "    elif 46 <= w_avg <= 75:\n",
    "        # Pause and request human review\n",
    "        human_input = interrupt(\n",
    "            \"Review required: The evaluation outcome is YELLOW.\\n\"\n",
    "            \"Please review reasoning, scores, and add additional context if needed. Decisions: [approve, edit, reject].\"\n",
    "        )\n",
    "        # Incorporate human input for final output (assume human_input is a dict with 'decision' and optional 'context')\n",
    "        if human_input['decision'] == \"approve\":\n",
    "            return \"green\"\n",
    "        elif human_input['decision'] == \"edit\":\n",
    "            # Recalculate based on human context (for simplicity, assume human provides new scores)\n",
    "            new_scheduling_score = human_input.get('scheduling_score', scheduling_score)\n",
    "            new_alignment_score = human_input.get('alignment_score', alignment_score)\n",
    "            new_workload_score = human_input.get('workload_score', workload_score)\n",
    "            return f\"Revaluate based on new scores: {new_scheduling_score}, {new_alignment_score}, {new_workload_score}\"\n",
    "        elif human_input['decision'] == \"reject\":\n",
    "            return \"red\"\n",
    "    else:\n",
    "        return \"green\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8574d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interrupt_agent_prompt_text = \"\"\"\n",
    "You are the study plan supervisor. For the provided study plan, call schedule_evaluator and alignment_evaluator as needed.\n",
    "Aggregate their results, call weighted_score_tool_with_interrupt, and provide an output containing:\n",
    "- scheduling_score, alignment_score, weighted_color\n",
    "- scheduling_reasoning, alignment_reasoning\n",
    "- overall_recommendation\n",
    "Additionally, evaluate a score based on the amount of courses selected called workload_score and make your evaluation based on that.\n",
    "Be critical and thorough in your evaluation, avoid being over optimistic.\n",
    "\"\"\"\n",
    "# Use from_messages to construct the ChatPromptTemplate (ChatPromptTemplate requires 'messages' when using constructor)\n",
    "interrupt_agent_prompt = ChatPromptTemplate.from_messages([('system',interrupt_agent_prompt_text),('human',\"Given a study plan: \\n{study_plan}\\n\")])\n",
    "interrupt_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[weighted_score_tool_with_interrupt, call_scheduling_agent, call_alignment_agent],\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\"weighted_score_tool_with_interrupt\": True}  # Only calls interrupt inside tool\n",
    "        ),\n",
    "    ],\n",
    "    checkpointer=InMemorySaver(),  # InMemory for testing; use persistent in production\n",
    "    response_format=StudyPlanEvaluation\n",
    ")\n",
    "\n",
    "chain_with_interrupt = interrupt_agent_prompt | interrupt_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcc1b321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool execution requires approval\n",
      "\n",
      "Tool: weighted_score_tool_with_interrupt\n",
      "Args: {'alignment_score': 70, 'scheduling_score': 60, 'workload_score': 80}\n",
      "Error retrieving final output: 'structured_response'\n",
      "{'__interrupt__': [Interrupt(value={'action_requests': [{'args': {'alignment_score': 90,\n",
      "                                                                  'scheduling_score': 90,\n",
      "                                                                  'workload_score': 90},\n",
      "                                                         'description': 'Tool '\n",
      "                                                                        'execution '\n",
      "                                                                        'requires '\n",
      "                                                                        'approval\\n'\n",
      "                                                                        '\\n'\n",
      "                                                                        'Tool: '\n",
      "                                                                        'weighted_score_tool_with_interrupt\\n'\n",
      "                                                                        'Args: '\n",
      "                                                                        \"{'alignment_score': \"\n",
      "                                                                        '90, '\n",
      "                                                                        \"'scheduling_score': \"\n",
      "                                                                        '90, '\n",
      "                                                                        \"'workload_score': \"\n",
      "                                                                        '90}',\n",
      "                                                         'name': 'weighted_score_tool_with_interrupt'}],\n",
      "                                    'review_configs': [{'action_name': 'weighted_score_tool_with_interrupt',\n",
      "                                                        'allowed_decisions': ['approve',\n",
      "                                                                              'edit',\n",
      "                                                                              'reject']}]},\n",
      "                             id='1d360163acb5d3bc77b181069a79774d')],\n",
      " 'messages': [SystemMessage(content='\\nYou are the study plan supervisor. For the provided study plan, call schedule_evaluator and alignment_evaluator as needed.\\nAggregate their results, call weighted_score_tool_with_interrupt, and provide an output containing:\\n- scheduling_score, alignment_score, weighted_color\\n- scheduling_reasoning, alignment_reasoning\\n- overall_recommendation\\nAdditionally, evaluate a score based on the amount of courses selected called workload_score and make your evaluation based on that.\\nBe critical and thorough in your evaluation, avoid being over optimistic.\\n', additional_kwargs={}, response_metadata={}, id='c6901169-197b-41f8-ba4d-3f00a79c65a6'),\n",
      "              HumanMessage(content='Given a study plan: \\nStudent Name: Leo\\nAcademic Program: Economics Major, International Business Minor\\nSemester: Fall 2025\\n\\nECO-101: Principles of Microeconomics\\n\\nECO-220: Intermediate Macroeconomics\\n\\nIB-400: International Business Strategy\\n\\nMTH-215: Linear Algebra (Mathematics Department)\\n\\nMKT-205: Principles of Marketing (Marketing Department)\\n\\nCHM-101: General Chemistry I (Chemistry Department)\\n\\nBUS-101: Introduction to Business Administration (Business Department)\\n\\n', additional_kwargs={}, response_metadata={}, id='73efd364-b815-428c-b61b-e57f6a194620'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'x68vq9p01', 'function': {'arguments': '{\"study_plan\":\"ECO-101: Principles of Microeconomics, ECO-220: Intermediate Macroeconomics, IB-400: International Business Strategy, MTH-215: Linear Algebra, MKT-205: Principles of Marketing, CHM-101: General Chemistry I, BUS-101: Introduction to Business Administration\"}', 'name': 'alignment_evaluator'}, 'type': 'function'}, {'id': 'gmsbm9rfd', 'function': {'arguments': '{\"study_plan\":\"ECO-101: Principles of Microeconomics, ECO-220: Intermediate Macroeconomics, IB-400: International Business Strategy, MTH-215: Linear Algebra, MKT-205: Principles of Marketing, CHM-101: General Chemistry I, BUS-101: Introduction to Business Administration\"}', 'name': 'schedule_evaluator'}, 'type': 'function'}, {'id': '4g7nyfm63', 'function': {'arguments': '{\"alignment_score\":70,\"scheduling_score\":60,\"workload_score\":80}', 'name': 'weighted_score_tool_with_interrupt'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 187, 'prompt_tokens': 844, 'total_tokens': 1031, 'completion_time': 0.324059284, 'prompt_time': 0.073858456, 'queue_time': 0.141553442, 'total_time': 0.39791774}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_21854da540', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--3a55ff10-f698-47a7-8cdc-33544c5a77f5-0', tool_calls=[{'name': 'alignment_evaluator', 'args': {'study_plan': 'ECO-101: Principles of Microeconomics, ECO-220: Intermediate Macroeconomics, IB-400: International Business Strategy, MTH-215: Linear Algebra, MKT-205: Principles of Marketing, CHM-101: General Chemistry I, BUS-101: Introduction to Business Administration'}, 'id': 'x68vq9p01', 'type': 'tool_call'}, {'name': 'schedule_evaluator', 'args': {'study_plan': 'ECO-101: Principles of Microeconomics, ECO-220: Intermediate Macroeconomics, IB-400: International Business Strategy, MTH-215: Linear Algebra, MKT-205: Principles of Marketing, CHM-101: General Chemistry I, BUS-101: Introduction to Business Administration'}, 'id': 'gmsbm9rfd', 'type': 'tool_call'}, {'name': 'weighted_score_tool_with_interrupt', 'args': {'alignment_score': 70, 'scheduling_score': 60, 'workload_score': 80}, 'id': '4g7nyfm63', 'type': 'tool_call'}], usage_metadata={'input_tokens': 844, 'output_tokens': 187, 'total_tokens': 1031}),\n",
      "              SystemMessage(content='\\nYou are the study plan supervisor. For the provided study plan, call schedule_evaluator and alignment_evaluator as needed.\\nAggregate their results, call weighted_score_tool_with_interrupt, and provide an output containing:\\n- scheduling_score, alignment_score, weighted_color\\n- scheduling_reasoning, alignment_reasoning\\n- overall_recommendation\\nAdditionally, evaluate a score based on the amount of courses selected called workload_score and make your evaluation based on that.\\nBe critical and thorough in your evaluation, avoid being over optimistic.\\n', additional_kwargs={}, response_metadata={}, id='e87b32c3-c459-4c05-96ed-77854f60a3db'),\n",
      "              HumanMessage(content=\"Given a study plan: \\nCommand(resume={'decisions': [{'decision': 'edit', 'context': 'increase alignment_score to 90 and scheduling_score to 90 and workload_score to 90'}]})\\n\", additional_kwargs={}, response_metadata={}, id='1b366f37-f960-438f-8efb-e8caf60c37fb'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '1vffkg2wr', 'function': {'arguments': '{\"alignment_score\":90,\"scheduling_score\":90,\"workload_score\":90}', 'name': 'weighted_score_tool_with_interrupt'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 1187, 'total_tokens': 1218, 'completion_time': 0.042575145, 'prompt_time': 0.105881962, 'queue_time': 0.085183676, 'total_time': 0.148457107}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--2470a49f-f8eb-452d-8d26-e8cf660e7249-0', tool_calls=[{'name': 'weighted_score_tool_with_interrupt', 'args': {'alignment_score': 90, 'scheduling_score': 90, 'workload_score': 90}, 'id': '1vffkg2wr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1187, 'output_tokens': 31, 'total_tokens': 1218})]}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "thread_id = \"interrupt-123\"\n",
    "yellow_plan = load_prompt(\"yellow_structured.txt\")\n",
    "# First invocation, agent runs until interrupt (if yellow)\n",
    "result = chain_with_interrupt.invoke(\n",
    "    {\"study_plan\": yellow_plan},\n",
    "    config={\"configurable\": {\"thread_id\": thread_id}}\n",
    ")\n",
    "\n",
    "if '__interrupt__' in result:\n",
    "    # Present review information to human, collect decision:\n",
    "    action = result['__interrupt__']\n",
    "    print(action[0].value['action_requests'][0]['description'])  # Display the interrupt message\n",
    "    # Simulate getting human review:\n",
    "    decision = input(\"Enter decision (approve/edit/reject): \")\n",
    "    context = \"\"\n",
    "    if decision == \"edit\":\n",
    "        context = input(\"Provide additional context: \")\n",
    "\n",
    "    # Resume agent using actual human input\n",
    "    resume_payload = {\"decision\": decision, \"context\": context}\n",
    "    review_result = chain_with_interrupt.invoke(\n",
    "        Command(resume={\"decisions\": [resume_payload]}),\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "    try:\n",
    "        pprint(review_result['structured_response'].model_dump())  # Will contain the final output after human review\n",
    "    except KeyError as e:\n",
    "        print(f\"Error retrieving final output: {e}\")\n",
    "        pprint(review_result)\n",
    "else:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
